{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d205749481fd995",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "_Author : Benjamin Ternot_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982617fb366bdf25",
   "metadata": {},
   "source": [
    "## I. Importing Libraries and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:23.266746Z",
     "start_time": "2024-12-01T16:06:18.032329Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from data_manager.datamanager import DataLoader, DataDisplayer, DataTransformer\n",
    "from models.model import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d29673-0b89-47ca-a313-f16e3844de54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:23.279857Z",
     "start_time": "2024-12-01T16:06:23.271818Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "\n",
    "LIBERATE_MEMORY = True # Delete intermediate object to free memory space\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = not TRAIN_MODEL\n",
    "MODEL_LOAD_PATH = \"models/trained_models/\"\n",
    "\n",
    "MULTI_CHANNEL = True\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "T = 1000\n",
    "DIM_MULTS = (1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875f290-9607-4eae-8c57-3a0bb302e882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:23.279857Z",
     "start_time": "2024-12-01T16:06:23.271818Z"
    }
   },
   "outputs": [],
   "source": [
    "current_datetime = datetime.datetime.now()\n",
    "SAVE_PATH = f\"images/{current_datetime.strftime('%Y-%m-%d-%H-%M')}_\" + '4-channels' if MULTI_CHANNEL else '1-channel'\n",
    "MODEL_SAVE_PATH = f\"models/trained_models/{current_datetime.strftime('%Y-%m-%d-%H-%M')}_\" + ('4-channels' if MULTI_CHANNEL else '1-channel') + \"_trained-unet.pt\"\n",
    "PARAMS_SAVE_PATH = f\"models/trained_models/{current_datetime.strftime('%Y-%m-%d-%H-%M')}_\" + ('4-channels' if MULTI_CHANNEL else '1-channel') + \"_params.txt\"\n",
    "INPUT_SAVE_PATH = SAVE_PATH + \"_batch_input.jpg\"\n",
    "OUTPUT_SAVE_PATH = SAVE_PATH + \"_sampling.jpg\"\n",
    "OUTPUT_ONE_HOT_SAVE_PATH = SAVE_PATH + \"_sampling_one-hot.jpg\"\n",
    "OUTPUT_GIF_SAVE_PATH = SAVE_PATH + \"_sampling.gif.png\"\n",
    "OUTPUT_GIF_ONE_HOT_SAVE_PATH = SAVE_PATH + \"_sampling_one-hot.gif.png\"\n",
    "\n",
    "SAVE_PARAMS = {\n",
    "    \"IMAGE_SIZE\": IMAGE_SIZE,\n",
    "    \"MULTI_CHANNEL\": MULTI_CHANNEL,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"T\": T,\n",
    "    \"DIM_MULTS\": DIM_MULTS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5011e-4051-41b8-a51c-4213178f4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write parameters to the file\n",
    "with open(PARAMS_SAVE_PATH, \"w\") as file:\n",
    "    for key, value in SAVE_PARAMS.items():\n",
    "        file.write(f\"{key} = {value}\\n\")\n",
    "print(f\"Parameters saved to '{PARAMS_SAVE_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ddae3d0ac6bef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:23.399047Z",
     "start_time": "2024-12-01T16:06:23.387791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modifier les couleurs des textes et des axes en fonction du th√®me de Jupyter\n",
    "DARK_BG = False\n",
    "\n",
    "if DARK_BG:\n",
    "    plt.rcParams['text.color'] = 'white'\n",
    "    plt.rcParams['axes.labelcolor'] = 'white'\n",
    "    plt.rcParams['xtick.color'] = 'white'\n",
    "    plt.rcParams['ytick.color'] = 'white'\n",
    "    plt.rcParams['axes.titlecolor'] = 'white'\n",
    "else:\n",
    "    plt.rcParams['text.color'] = 'black'\n",
    "    plt.rcParams['axes.labelcolor'] = 'black'\n",
    "    plt.rcParams['xtick.color'] = 'black'\n",
    "    plt.rcParams['ytick.color'] = 'black'\n",
    "    plt.rcParams['axes.titlecolor'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb50f2364e0904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:23.429363Z",
     "start_time": "2024-12-01T16:06:23.415122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the root data folder\n",
    "root_data_folder = os.path.join(os.path.dirname(os.getcwd()), 'database')\n",
    "\n",
    "# Define the sub path to the folders containing the data\n",
    "data_sub_folders = {\n",
    "    \"train\": \"training\",\n",
    "    \"test\": \"testing\",\n",
    "}\n",
    "\n",
    "# Define the mapping from group labels to diagnostic classes\n",
    "group_map = {\n",
    "    \"NOR\": \"Healthy control\",\n",
    "    \"MINF\": \"Myocardial infarction\",\n",
    "    \"DCM\": \"Dilated cardiomyopathy\",\n",
    "    \"HCM\": \"Hypertrophic cardiomyopathy\",\n",
    "    \"RV\": \"Abnormal right ventricle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc2e0bfa30b761",
   "metadata": {},
   "source": [
    "## II. Data Loading and Displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d294d5b1f177b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:31.269699Z",
     "start_time": "2024-12-01T16:06:23.448232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataLoader instance\n",
    "data_loader = DataLoader(root_data_folder)\n",
    "\n",
    "# Load the data\n",
    "for key, sub_folder in data_sub_folders.items():\n",
    "    data_loader.load_data(sub_folder, name=key, store=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde20899bddf38f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:31.359774Z",
     "start_time": "2024-12-01T16:06:31.287779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the DataDisplayer with the DataLoader instance\n",
    "data_displayer = DataDisplayer(data_loader, group_map=group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87b8415ceccc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:34.616441Z",
     "start_time": "2024-12-01T16:06:31.383496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display as an arborescence the dataloader.data dictionary\n",
    "print(data_displayer.display_data_arborescence('data_loader.data', max_keys={1: 2}))\n",
    "\n",
    "# Display some examples\n",
    "data_displayer.display_examples(nb_examples=1, per_combination=True, sort_by=('data_name', 'group', 'id'), format_sep=('#' * 90, '-' * 60, ''), format_categories=('{} data :\\n', '{} :', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9527085abee0e5e",
   "metadata": {},
   "source": [
    "## III. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b519c000a489d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:34.769983Z",
     "start_time": "2024-12-01T16:06:34.635537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show repartition of shape of the images\n",
    "shape_dict = {}\n",
    "for dataset_key, dataset in data_loader.data.items():\n",
    "    for patient, patient_data in dataset.items():\n",
    "        for image_name, image in patient_data['image_data'].items():\n",
    "            shape = image.shape\n",
    "            if shape not in shape_dict:\n",
    "                shape_dict[shape] = 1\n",
    "            else:\n",
    "                shape_dict[shape] += 1\n",
    "\n",
    "print(\n",
    "    f'There are {len(shape_dict)} different shapes in the data'\n",
    "    f'\\nX axis varies from {min([shape[0] for shape in shape_dict.keys()])} to {max([shape[0] for shape in shape_dict.keys()])}'\n",
    "    f'\\nY axis varies from {min([shape[1] for shape in shape_dict.keys()])} to {max([shape[1] for shape in shape_dict.keys()])}'\n",
    "    f'\\nZ axis varies from {min([shape[2] for shape in shape_dict.keys()])} to {max([shape[2] for shape in shape_dict.keys()])}'\n",
    "    f'\\nAspect ratio (X/Y) varies from {min([shape[0] / shape[1] for shape in shape_dict.keys()])} to {max([shape[0] / shape[1] for shape in shape_dict.keys()])}'\n",
    ")\n",
    "# plot the repartition of the aspect ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([shape[0] / shape[1] for shape in shape_dict.keys()], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Aspect ratio distribution')\n",
    "plt.xlabel('Aspect ratio (X/Y)')\n",
    "plt.ylabel('Number of images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6845a04e84517",
   "metadata": {},
   "source": [
    "## IV. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bffb491ad2207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:34.814722Z",
     "start_time": "2024-12-01T16:06:34.801406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate the data transformer\n",
    "data_transformer = DataTransformer(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464604d4324687d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:36.747499Z",
     "start_time": "2024-12-01T16:06:34.842484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resize the images to a common shape\n",
    "target_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "data_transformer.crop_and_resize(target_shape=target_shape, padding=0.2, output_key='image_resized_data', keep_3d_consistency=False, link_gt_to_data=False, create_channels_from_gt=MULTI_CHANNEL, image_names=['ED_gt', 'ES_gt'])\n",
    "\n",
    "# Display tree of the data after resizing\n",
    "print(data_displayer.display_data_arborescence('data_loader.data', max_keys=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2d1e5a2464ab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:38.990834Z",
     "start_time": "2024-12-01T16:06:36.776269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display some examples for the resized images\n",
    "data_displayer.display_examples(image_type='image_resized_data', image_names=['ED_gt', 'ES_gt'], nb_examples=1, per_combination=True, sort_by=('data_name', 'group', 'id'), format_sep=('#' * 90, '-' * 60, ''), format_categories=('{} data :\\n', '{} :', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdb030138a9e3f",
   "metadata": {},
   "source": [
    "## V. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0aa06-5be7-4dfd-9227-ad5a7125cab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:58.359126Z",
     "start_time": "2024-12-01T16:06:39.023148Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_images_gt = data_loader.extract_specific_images(image_types=['image_resized_data'], image_names=['ED_gt', 'ES_gt'])\n",
    "rotated_images = []\n",
    "nb_rotations = 7\n",
    "max_angle = 45\n",
    "for angle in tqdm(np.linspace(-max_angle, max_angle, nb_rotations), desc='Rotating images'):\n",
    "    if angle == 0:\n",
    "        rotated_images.append(transformed_images_gt)\n",
    "    else:\n",
    "        rotated_images.append(DataTransformer.rotate_images(angle=angle, images=transformed_images_gt, has_channels=transformed_images_gt[0].shape[0] > 3))\n",
    "# concatenate all lists\n",
    "rotated_images = [images for sublist in rotated_images for images in sublist]\n",
    "print(f'Number of images after rotation: {len(rotated_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aafea8aa19d3f",
   "metadata": {},
   "source": [
    "## Model Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a218f4165295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:58.492757Z",
     "start_time": "2024-12-01T16:06:58.480758Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:3\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded19d04153026f6",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b286be565fbf0a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:58.534622Z",
     "start_time": "2024-12-01T16:06:58.518571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436155a24fe0cd5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:58.685423Z",
     "start_time": "2024-12-01T16:06:58.566583Z"
    }
   },
   "outputs": [],
   "source": [
    "training_images = data_transformer.slice_depth_images(rotated_images)\n",
    "# Keep a multiple of batch_size images\n",
    "training_images = training_images[:(len(training_images)//batch_size)*batch_size]\n",
    "training_images = DataTransformer.one_hot_encode_batch(training_images)\n",
    "print(f'Number of images after slicing: {len(training_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad57af485d507385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:06:59.912854Z",
     "start_time": "2024-12-01T16:06:58.731542Z"
    }
   },
   "outputs": [],
   "source": [
    "if LIBERATE_MEMORY:\n",
    "    del shape_dict\n",
    "    del data_loader\n",
    "    del data_transformer\n",
    "    del data_displayer\n",
    "    del transformed_images_gt\n",
    "    del rotated_images\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636dc08f52e90aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:07:00.067416Z",
     "start_time": "2024-12-01T16:06:59.943891Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader, channels, image_size, len_dataloader = TorchDataLoader(dataset=training_images, batch_size=batch_size, shuffle=True), training_images[0].shape[0], target_shape[0], len(training_images)//batch_size\n",
    "\n",
    "def format_batch_shape_string(batch_shape: tuple[int]):\n",
    "    return \"{} image(s), {} channel(s), {} row(s), {} column(s)\".format(*batch_shape)\n",
    "\n",
    "# Get a random batch from the train_dataloader without converting it to a list\n",
    "random_batch_index = random.randint(0, len(train_dataloader) - 1)\n",
    "# Use iter and advance to the random index\n",
    "batch_iterator = iter(train_dataloader)\n",
    "for _ in range(random_batch_index):\n",
    "    next(batch_iterator)  # Advance to the random batch\n",
    "batch_image = next(batch_iterator)  # Get the random batch\n",
    "print(f\"Shape of each batch: [{format_batch_shape_string(batch_image.shape)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425be8a1dcd23bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:07:00.156602Z",
     "start_time": "2024-12-01T16:07:00.143420Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_im(images):\n",
    "    shape = images.shape\n",
    "    images = images.view(shape[0], -1)\n",
    "    images -= images.min(1, keepdim=True)[0]\n",
    "    images /= images.max(1, keepdim=True)[0]\n",
    "    return images.view(shape)\n",
    "\n",
    "def show_images(batch, filename=None, one_hot_encode=False):\n",
    "    image = np.array(torch.permute(make_grid(batch, nrow=int(np.sqrt(len(batch)))), (1,2,0)).cpu())\n",
    "    if image.shape[-1] > 3:\n",
    "        if one_hot_encode:\n",
    "            image = DataDisplayer.one_hot_encode(image)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        axes[0].imshow(image[..., 1:4])\n",
    "        axes[0].set_title(\"RGB Channels\")\n",
    "        axes[0].axis('off')\n",
    "        im_bg = axes[1].imshow(image[..., 0], cmap='viridis')\n",
    "        axes[1].set_title(\"Background Intensity\")\n",
    "        axes[1].axis('off')\n",
    "        cbar = fig.colorbar(im_bg, ax=axes[1], orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label(\"Intensity\")\n",
    "        plt.tight_layout()\n",
    "        if filename:\n",
    "            fig.savefig(filename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        if filename:\n",
    "            plt.savefig(filename)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7700f325e3b946e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:07:00.611210Z",
     "start_time": "2024-12-01T16:07:00.186996Z"
    }
   },
   "outputs": [],
   "source": [
    "show_images(batch_image[:], INPUT_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db1a511bac16a7",
   "metadata": {},
   "source": [
    "## Forward Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cb872778aa66",
   "metadata": {},
   "source": [
    "### Beta Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e49fd537562e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different type of beta schedule\n",
    "def linear_beta_schedule(timesteps, beta_start = 0.0001, beta_end = 0.02):\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e92a16935546a",
   "metadata": {},
   "source": [
    "### Constants calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb55bf5432462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get alphas and betas\n",
    "def get_alph_bet(timesteps, schedule=cosine_beta_schedule):\n",
    "\n",
    "    # define beta\n",
    "    betas = schedule(timesteps)\n",
    "\n",
    "    # define alphas\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0) # cumulative product of alpha\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)  # corresponding to the prev const\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "    # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "    # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "    const_dict = {\n",
    "        'betas': betas,\n",
    "        'sqrt_recip_alphas': sqrt_recip_alphas,\n",
    "        'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n",
    "        'sqrt_one_minus_alphas_cumprod': sqrt_one_minus_alphas_cumprod,\n",
    "        'posterior_variance': posterior_variance\n",
    "    }\n",
    "\n",
    "    return const_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fea926e66a372b",
   "metadata": {},
   "source": [
    "### Definition of $q(x_t | x_{0})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90921e856f10e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the values needed for time t\n",
    "def extract(constants, batch_t, x_shape):\n",
    "    diffusion_batch_size = batch_t.shape[0]\n",
    "\n",
    "    # get a list of the appropriate constants of each timesteps\n",
    "    out = constants.gather(-1, batch_t.cpu())\n",
    "\n",
    "    return out.reshape(diffusion_batch_size, *((1,) * (len(x_shape) - 1))).to(batch_t.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81a878c87f82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward diffusion (using the nice property)\n",
    "def q_sample(constants_dict, batch_x0, batch_t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(batch_x0)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(constants_dict['sqrt_alphas_cumprod'], batch_t, batch_x0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        constants_dict['sqrt_one_minus_alphas_cumprod'], batch_t, batch_x0.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * batch_x0 + sqrt_one_minus_alphas_cumprod_t * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0702c267fc91d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "const_linear_dict = get_alph_bet(T, schedule=linear_beta_schedule)\n",
    "const_cosine_dict = get_alph_bet(T, schedule=cosine_beta_schedule)\n",
    "\n",
    "batch_t = torch.arange(batch_size)*(T//batch_size)  # get a range of timesteps from 0 to T\n",
    "print(f\"timesteps: {batch_t}\")\n",
    "noisy_batch_linear = q_sample(const_linear_dict, batch_image, batch_t, noise=None)\n",
    "noisy_batch_cosine = q_sample(const_cosine_dict, batch_image, batch_t, noise=None)\n",
    "\n",
    "print(\"Original images:\")\n",
    "show_images(batch_image[:])\n",
    "\n",
    "print(\"Noised images with linear shedule:\")\n",
    "show_images(noisy_batch_linear[:])\n",
    "\n",
    "print(\"Noised images with cosine shedule:\")\n",
    "show_images(noisy_batch_cosine[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377aec8b61647315",
   "metadata": {},
   "source": [
    "## Model Diffusion Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6fd685a4586af",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410812cd26a7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "        dim=image_size,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=DIM_MULTS,\n",
    "        channels=channels,\n",
    "        with_time_emb=True,\n",
    "        convnext_mult=2,\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7a3e284aa4877",
   "metadata": {},
   "source": [
    "### Definition of $ p_{\\theta}(x_{t-1}|x_t) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82479783c6d0330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(constants_dict, batch_xt, predicted_noise, batch_t):\n",
    "    # We first get every constants needed and send them in right device\n",
    "    betas_t = extract(constants_dict['betas'], batch_t, batch_xt.shape).to(batch_xt.device)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        constants_dict['sqrt_one_minus_alphas_cumprod'], batch_t, batch_xt.shape\n",
    "    ).to(batch_xt.device)\n",
    "    sqrt_recip_alphas_t = extract(\n",
    "        constants_dict['sqrt_recip_alphas'], batch_t, batch_xt.shape\n",
    "    ).to(batch_xt.device)\n",
    "\n",
    "    # Equation 11 in the ddpm paper\n",
    "    # Use predicted noise to predict the mean (mu theta)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        batch_xt - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    # We have to be careful to not add noise if we want to predict the final image\n",
    "    predicted_image = torch.zeros(batch_xt.shape).to(batch_xt.device)\n",
    "    t_zero_index = (batch_t == torch.zeros(batch_t.shape).to(batch_xt.device))\n",
    "\n",
    "    # Algorithm 2 line 4, we add noise when timestep is not 1:\n",
    "    posterior_variance_t = extract(constants_dict['posterior_variance'], batch_t, batch_xt.shape)\n",
    "    noise = torch.randn_like(batch_xt)  # create noise, same shape as batch_x\n",
    "    predicted_image[~t_zero_index] = model_mean[~t_zero_index] + (\n",
    "        torch.sqrt(posterior_variance_t[~t_zero_index]) * noise[~t_zero_index]\n",
    "    )\n",
    "\n",
    "    # If t=1 we don't add noise to mu\n",
    "    predicted_image[t_zero_index] = model_mean[t_zero_index]\n",
    "\n",
    "    return predicted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58990b568b64c252",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de6efc763831ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2 (including returning all images)\n",
    "@torch.no_grad()\n",
    "def sampling(model, shape, T, constants_dict):\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    batch_xt = torch.randn(shape, device=DEVICE)\n",
    "\n",
    "    batch_t = torch.ones(shape[0]) * T  # create a vector with batch-size time the timestep\n",
    "    batch_t = batch_t.type(torch.int64).to(DEVICE)\n",
    "\n",
    "    imgs = []\n",
    "\n",
    "    for t in tqdm(reversed(range(0, T)), desc='sampling loop time step', total=T):\n",
    "        batch_t -= 1\n",
    "        predicted_noise = model(batch_xt, batch_t)\n",
    "\n",
    "        batch_xt = p_sample(constants_dict, batch_xt, predicted_noise, batch_t)\n",
    "\n",
    "        imgs.append(batch_xt.cpu())\n",
    "\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988aef43935a4bc5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5d760369e5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, channels, image_size, len_dataloader = TorchDataLoader(dataset=training_images, batch_size=batch_size), training_images[0].shape[0], target_shape[0], len(training_images)//batch_size\n",
    "constants_dict = get_alph_bet(T, schedule=linear_beta_schedule)\n",
    "\n",
    "epochs = EPOCHS\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847753b9add97c89",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8689e089ee973",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size_iter = batch.shape[0]\n",
    "            batch_image = batch.to(DEVICE)\n",
    "\n",
    "            # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "            batch_t = torch.randint(0, T, (batch_size_iter,), device=DEVICE).long()\n",
    "\n",
    "            noise = torch.randn_like(batch_image)\n",
    "\n",
    "            x_noisy = q_sample(constants_dict, batch_image, batch_t, noise=noise)\n",
    "            predicted_noise = model(x_noisy, batch_t)\n",
    "\n",
    "            loss = criterion(noise, predicted_noise)\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9ff27-bfe8-49fb-a9cc-4b7cd98b886f",
   "metadata": {},
   "source": [
    "## Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bbed0-5fd3-41f1-a672-3c8bc01c5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL :\n",
    "    model = Unet(\n",
    "            dim=image_size,\n",
    "            init_dim=None,\n",
    "            out_dim=None,\n",
    "            dim_mults=DIM_MULTS,\n",
    "            channels=channels,\n",
    "            with_time_emb=True,\n",
    "            convnext_mult=2,\n",
    "        ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MODEL_LOAD_PATH, weights_only=True, map_location=torch.device(DEVICE)))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90832d2ab50e140b",
   "metadata": {},
   "source": [
    "## View of the diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da07f4319d099e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(frame_list, filename, step=5, one_hot_encode=True):    \n",
    "    to_pil = ToPILImage()\n",
    "    \n",
    "    frames = [np.array(torch.permute(make_grid(normalize_im(tens_im), nrow=int(np.sqrt(batch_size))), (1,2,0)).cpu()) for tens_im in frame_list]\n",
    "    frames_to_include = frames[0] + frames[1::step]\n",
    "    if frames[-1] not in frames_to_include:\n",
    "        frames_to_include.append(frames[-1])\n",
    "    if frames_to_include[0].shape[-1] > 3:\n",
    "        if one_hot_encode:\n",
    "            frames_to_include = [DataDisplayer.one_hot_encode(frame) for frame in frames_to_include]\n",
    "        frames_to_include = [frame[..., 1:4] for frame in frames_to_include]\n",
    "    frames_pil = [to_pil(frame) for frame in frames_to_include]\n",
    "    frame_one = frames_pil[0]\n",
    "    \n",
    "    frame_one.save(filename, format=\"GIF\", append_images=frames_pil[1::], save_all=True, duration=10, loop=0)\n",
    "\n",
    "    return IPython.display.Image(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033f5a6-b372-4cb0-9521-45fd26613bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"check generation:\")\n",
    "list_gen_imgs = sampling(model, (batch_size, channels, image_size, image_size), T, constants_dict)\n",
    "\n",
    "show_images(list_gen_imgs[-1][:], OUTPUT_SAVE_PATH, one_hot_encode=False)\n",
    "show_images(list_gen_imgs[-1][:], OUTPUT_ONE_HOT_SAVE_PATH, one_hot_encode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2aaf1-28a2-489d-8b91-37e2385b6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_gif(list_gen_imgs, OUTPUT_GIF_ONE_HOT_SAVE_PATH, step=20))\n",
    "display(make_gif(list_gen_imgs, OUTPUT_GIF_SAVE_PATH, step=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6dd7bb-9a14-4c6a-bf49-863a01721ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "for _ in tqdm(range(100), desc='Generating images'):\n",
    "    sample = sampling(model, (batch_size, channels, image_size, image_size), T, constants_dict)\n",
    "    for image in sample[-1][:]:\n",
    "        generated_images.append(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
